{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65582e22-db40-45dd-a218-02b7cc183a5c",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "import pickle\n",
    "\n",
    "from fuzzy_expert.variable import FuzzyVariable\n",
    "from fuzzy_expert.rule import FuzzyRule\n",
    "from fuzzy_expert.inference import DecompositionalInference\n",
    "from retry_requests import retry\n",
    "from datetime import date, timedelta, datetime\n",
    "from scipy.stats import linregress\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bbe4b8f-3230-4c94-8563-e027918f869b",
   "metadata": {},
   "source": [
    "def pre_processing(data):\n",
    "\n",
    "    # Columns Selection and Formatting\n",
    "    \n",
    "    df_rovere = data[['reading_id', 'timestamp', 'sensor_id', 'value', 'description', 'group_id']].copy()\n",
    "    df_rovere[['reading_id', 'sensor_id', 'description', 'group_id']] = df_rovere[['reading_id', 'sensor_id', 'description', 'group_id']].astype(str)\n",
    "    df_rovere['timestamp'] = pd.to_datetime(df_rovere['timestamp']).dt.floor('D').dt.date\n",
    "    df_rovere['value'] = df_rovere['value'].astype(float)\n",
    "\n",
    "    tens_30 = ['72', '76', '73', '74', '61', '63', '67', '65']\n",
    "    tens_60 = ['71', '69', '75', '70', '62', '64', '68', '66']\n",
    "    tens_all = tens_30 + tens_60\n",
    "    \n",
    "    df_rovere.loc[df_rovere['description'] == 'tensiometer', 'description'] = 'Tensiometer'\n",
    "    df_rovere.loc[df_rovere['description'] == 'irrigation', 'description'] = 'Irrigation'\n",
    "\n",
    "    \n",
    "    # Duplication\n",
    "    \n",
    "    condition_not_in_list = ~df_rovere['sensor_id'].isin(tens_30)\n",
    "    df_dup = df_rovere[condition_not_in_list]\n",
    "    df_dup['group_id'] = df_dup['group_id'] + '_dup'\n",
    "\n",
    "    df_rovere = df_rovere[~df_rovere['sensor_id'].isin(tens_60)]\n",
    "    df_rovere = pd.concat([df_rovere, df_dup], ignore_index=True)\n",
    "    df_rovere.sort_values(by=['group_id', 'timestamp'], inplace=True)\n",
    "    df_rovere.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    \n",
    "    # Grouping and Creation of Summary Values\n",
    "    \n",
    "    df_group = df_rovere.groupby(['timestamp', 'description', 'sensor_id', 'group_id']).agg({'value': ['min', 'max', 'mean', 'median', 'sum']}).reset_index()\n",
    "    df_group.columns = ['timestamp', 'description', 'sensor_id', 'group_id', 'val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']\n",
    "\n",
    "    \n",
    "    # Pivoting\n",
    "    \n",
    "    df_pivot = df_group.pivot(index=['timestamp', 'group_id'], columns='description', values=['val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']).reset_index()\n",
    "    df_pivot.columns = ['date', 'group_id'] + [f\"{agg}_{feature}\" for agg in ['min', 'max', 'avg', 'med', 'sum'] for feature in ['hum', 'temp', 'solar', 'wind', 'irr', 'rain', 'tens']]\n",
    "\n",
    "    df = df_pivot.reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    # Sensor ID Mapping\n",
    "    \n",
    "    group_id_mapping = {str(i): str(j) for i, j in zip(range(1, 9), tens_30)}\n",
    "    group_id_mapping.update({str(i) + '_dup': str(j) for i, j in zip(range(1, 9), tens_60)})\n",
    "    df['group_id'] = df['group_id'].replace(group_id_mapping)\n",
    "    df = df.rename(columns={'group_id': 'sensor_id'})\n",
    "\n",
    "    df = df[['sensor_id', 'date', 'avg_tens', 'max_temp', 'avg_hum', 'avg_solar', 'sum_rain', 'sum_irr']]\n",
    "    df = df[['sensor_id', 'date', 'avg_tens'] + [col for col in df.columns if col not in ['sensor_id', 'date', 'avg_tens']]]\n",
    "    df = df.sort_values(by=['sensor_id', 'date']).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    # Imputation of Missing Values\n",
    "    \n",
    "    float_columns = df.select_dtypes(include=['float']).columns\n",
    "    df[float_columns] = df[float_columns].interpolate(method='linear', limit_direction='both')\n",
    "    \n",
    "\n",
    "    # Shifting Values using the previous 3 Days\n",
    "    \n",
    "    ids = df['sensor_id']\n",
    "    dates = df['date']\n",
    "    X = df.drop(columns=['date', 'sensor_id'])\n",
    "    X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "    \n",
    "    X['date'] = dates\n",
    "    dates_to_remove = [date(2023, 4, 28), date(2023, 4, 29), date(2023, 4, 30)]\n",
    "    X = X[~X['date'].isin(dates_to_remove)].reset_index(drop=True)\n",
    "    X = X.drop(columns='date')\n",
    "    \n",
    "    y = df[['sensor_id', 'date', 'avg_tens']]\n",
    "    y = y[~y['date'].isin(dates_to_remove)].reset_index(drop=True)\n",
    "    \n",
    "    df_merged = pd.concat([y, X], axis=1)\n",
    "    df = df_merged[df_merged['sensor_id'].isin(tens_30)]\n",
    "    \n",
    "    return df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc8a004d-4c65-4c3e-9eb8-ab5fd8da5d2d",
   "metadata": {},
   "source": [
    "df = pd.read_json('row_data_rovere.json')\n",
    "df = pre_processing(df)\n",
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "566c5bf2-148a-4369-b38f-fc7484e7a7ac",
   "metadata": {},
   "source": [
    "# Crea il DataFrame df_test vuoto\n",
    "df_test = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "# Inizializza un contatore per tenere traccia delle righe\n",
    "counter = 0\n",
    "\n",
    "# Itera attraverso le righe di df\n",
    "for index, row in df.iterrows():\n",
    "    # Se il contatore Ã¨ multiplo di 10 (ogni 10 osservazioni)\n",
    "    if counter % 10 == 0:\n",
    "        # Aggiungi la riga a df_test usando il metodo loc\n",
    "        df_test.loc[len(df_test)] = row\n",
    "        # Rimuovi la riga da df usando l'indice\n",
    "        df = df.drop(index)\n",
    "    # Incrementa il contatore\n",
    "    counter += 1\n",
    "\n",
    "# Resetta gli indici di df\n",
    "df = df.reset_index(drop=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0eee221-bcb0-4ee5-9163-f86a14a234cc",
   "metadata": {},
   "source": [
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "275bc14f-648f-404e-b74b-8d27a2bc1bb1",
   "metadata": {},
   "source": [
    "df_test"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40555be1-9e98-47e4-a963-8ef3383a5174",
   "metadata": {},
   "source": [
    "# Rimuovi le colonne 'sensor_id' e 'date' da df\n",
    "df = df.drop(columns=['sensor_id', 'date'])\n",
    "\n",
    "# Rimuovi le colonne 'sensor_id' e 'date' da df_test\n",
    "df_test = df_test.drop(columns=['sensor_id', 'date'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f82f4a9-bb68-437d-a344-7b785db955d7",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Dividi il dataset df in training e validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.drop(columns=['avg_tens']), df['avg_tens'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Inizializza il modello di regressione lineare\n",
    "model = LinearRegression()\n",
    "\n",
    "# Addestra il modello sul training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Valuta il modello sul training set\n",
    "train_predictions = model.predict(X_train)\n",
    "train_error = mean_squared_error(y_train, train_predictions)\n",
    "\n",
    "# Valuta il modello sul validation set\n",
    "val_predictions = model.predict(X_val)\n",
    "val_error = mean_squared_error(y_val, val_predictions)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(\"Train Error:\", train_error)\n",
    "print(\"Validation Error:\", val_error)\n",
    "\n",
    "# Applica il modello sul dataset df_test\n",
    "X_test = df_test.drop(columns=['avg_tens'])  # Assicurati di adattare questa parte in base alla struttura del tuo df_test\n",
    "y_test = df_test['avg_tens']\n",
    "test_predictions = model.predict(X_test)\n",
    "test_error = mean_squared_error(y_test, test_predictions)\n",
    "\n",
    "# Stampa l'errore sul dataset df_test\n",
    "print(\"Test Error on df_test:\", test_error)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50fb7015-1245-48a6-89ed-13dd3a2ac5a1",
   "metadata": {},
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Calcola l'RMSE sul training set\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_predictions))\n",
    "\n",
    "# Calcola l'RMSE sul validation set\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
    "\n",
    "# Stampa i risultati\n",
    "print(\"Train RMSE:\", train_rmse)\n",
    "print(\"Validation RMSE:\", val_rmse)\n",
    "\n",
    "# Calcola l'RMSE sul dataset df_test\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "\n",
    "# Stampa l'RMSE sul dataset df_test\n",
    "print(\"Test RMSE on df_test:\", test_rmse)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d348389-690e-46ed-b42c-0d09fa2777c5",
   "metadata": {},
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Calcola il MAE sul training set\n",
    "train_mae = mean_absolute_error(y_train, train_predictions)\n",
    "\n",
    "# Calcola il MAE sul validation set\n",
    "val_mae = mean_absolute_error(y_val, val_predictions)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(\"Train MAE:\", train_mae)\n",
    "print(\"Validation MAE:\", val_mae)\n",
    "\n",
    "# Calcola il MAE sul dataset df_test\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "\n",
    "# Stampa il MAE sul dataset df_test\n",
    "print(\"Test MAE on df_test:\", test_mae)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d38b0ba-16c6-413d-92e4-66888aa80d2b",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Inizializza il modello di regressione lineare\n",
    "model = LinearRegression()\n",
    "\n",
    "# Definisci il numero di fold per la cross-validation\n",
    "num_folds = 5\n",
    "\n",
    "# Definisci l'oggetto KFold\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Esegui la cross-validation e ottieni gli errori su training e validation set\n",
    "train_errors = []\n",
    "val_errors = []\n",
    "\n",
    "for train_index, val_index in kf.split(df.drop(columns=['avg_tens'])):\n",
    "    X_train, X_val = df.drop(columns=['avg_tens']).iloc[train_index], df.drop(columns=['avg_tens']).iloc[val_index]\n",
    "    y_train, y_val = df['avg_tens'].iloc[train_index], df['avg_tens'].iloc[val_index]\n",
    "\n",
    "    # Addestra il modello sul training set\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Valuta il modello sul training set\n",
    "    train_predictions = model.predict(X_train)\n",
    "    train_error = mean_squared_error(y_train, train_predictions)\n",
    "    train_errors.append(train_error)\n",
    "\n",
    "    # Valuta il modello sul validation set\n",
    "    val_predictions = model.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, val_predictions)\n",
    "    val_errors.append(val_error)\n",
    "\n",
    "# Calcola la media degli errori su training e validation set\n",
    "mean_train_error = np.mean(train_errors)\n",
    "mean_val_error = np.mean(val_errors)\n",
    "\n",
    "# Stampa i risultati della cross-validation\n",
    "print(\"Mean Train Error:\", mean_train_error)\n",
    "print(\"Mean Validation Error:\", mean_val_error)\n",
    "\n",
    "# Calcola l'errore sul dataset df_test\n",
    "test_predictions = model.predict(df_test.drop(columns=['avg_tens']))\n",
    "test_error = mean_squared_error(df_test['avg_tens'], test_predictions)\n",
    "\n",
    "# Stampa l'errore sul dataset df_test\n",
    "print(\"Test Error on df_test:\", test_error)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47d4842c-1547-450a-92cf-48a6db983546",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Inizializza il modello di regressione lineare\n",
    "model = LinearRegression()\n",
    "\n",
    "# Definisci il numero di fold per la cross-validation\n",
    "num_folds = 5\n",
    "\n",
    "# Definisci l'oggetto KFold\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Liste per memorizzare gli errori su training e validation set\n",
    "train_errors = []\n",
    "val_errors = []\n",
    "\n",
    "for train_index, val_index in kf.split(df.drop(columns=['avg_tens'])):\n",
    "    X_train, X_val = df.drop(columns=['avg_tens']).iloc[train_index], df.drop(columns=['avg_tens']).iloc[val_index]\n",
    "    y_train, y_val = df['avg_tens'].iloc[train_index], df['avg_tens'].iloc[val_index]\n",
    "\n",
    "    # Addestra il modello sul training set\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Valuta il modello sul training set\n",
    "    train_predictions = model.predict(X_train)\n",
    "    train_error = np.sqrt(mean_squared_error(y_train, train_predictions))  # RMSE\n",
    "    train_errors.append(train_error)\n",
    "\n",
    "    # Valuta il modello sul validation set\n",
    "    val_predictions = model.predict(X_val)\n",
    "    val_error = np.sqrt(mean_squared_error(y_val, val_predictions))  # RMSE\n",
    "    val_errors.append(val_error)\n",
    "\n",
    "# Calcola la media degli errori su training e validation set\n",
    "mean_train_rmse = np.mean(train_errors)\n",
    "mean_val_rmse = np.mean(val_errors)\n",
    "\n",
    "# Stampa i risultati dell'RMSE\n",
    "print(\"Mean Train RMSE:\", mean_train_rmse)\n",
    "print(\"Mean Validation RMSE:\", mean_val_rmse)\n",
    "\n",
    "# Calcola il MAE su training e validation set\n",
    "train_errors = []\n",
    "val_errors = []\n",
    "\n",
    "for train_index, val_index in kf.split(df.drop(columns=['avg_tens'])):\n",
    "    X_train, X_val = df.drop(columns=['avg_tens']).iloc[train_index], df.drop(columns=['avg_tens']).iloc[val_index]\n",
    "    y_train, y_val = df['avg_tens'].iloc[train_index], df['avg_tens'].iloc[val_index]\n",
    "\n",
    "    # Addestra il modello sul training set\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Valuta il modello sul training set\n",
    "    train_predictions = model.predict(X_train)\n",
    "    train_error = mean_absolute_error(y_train, train_predictions)  # MAE\n",
    "    train_errors.append(train_error)\n",
    "\n",
    "    # Valuta il modello sul validation set\n",
    "    val_predictions = model.predict(X_val)\n",
    "    val_error = mean_absolute_error(y_val, val_predictions)  # MAE\n",
    "    val_errors.append(val_error)\n",
    "\n",
    "# Calcola la media degli errori su training e validation set\n",
    "mean_train_mae = np.mean(train_errors)\n",
    "mean_val_mae = np.mean(val_errors)\n",
    "\n",
    "# Stampa i risultati del MAE\n",
    "print(\"Mean Train MAE:\", mean_train_mae)\n",
    "print(\"Mean Validation MAE:\", mean_val_mae)\n",
    "\n",
    "# Calcola l'errore sul dataset df_test\n",
    "test_predictions = model.predict(df_test.drop(columns=['avg_tens']))\n",
    "test_error = np.sqrt(mean_squared_error(df_test['avg_tens'], test_predictions))  # RMSE\n",
    "\n",
    "# Stampa l'RMSE sul dataset df_test\n",
    "print(\"Test RMSE on df_test:\", test_error)\n",
    "\n",
    "# Calcola il MAE sul dataset df_test\n",
    "test_error_mae = mean_absolute_error(df_test['avg_tens'], test_predictions)  # MAE\n",
    "\n",
    "# Stampa il MAE sul dataset df_test\n",
    "print(\"Test MAE on df_test:\", test_error_mae)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4962990-6e66-4f6f-8fd2-bfd181fdfa06",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Inizializza il modello XGBoost\n",
    "model = XGBRegressor()\n",
    "\n",
    "# Definisci la griglia di iperparametri da esplorare\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Definisci il numero di fold per la cross-validation\n",
    "num_folds = 80\n",
    "\n",
    "# Definisci l'oggetto KFold\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Crea un oggetto GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=kf)\n",
    "\n",
    "# Esegui la ricerca degli iperparametri\n",
    "grid_result = grid_search.fit(df.drop(columns=['avg_tens']), df['avg_tens'])\n",
    "\n",
    "# Stampa i migliori iperparametri trovati\n",
    "print(\"Best Parameters:\", grid_result.best_params_)\n",
    "\n",
    "# Ottieni il miglior modello\n",
    "best_model = grid_result.best_estimator_\n",
    "\n",
    "# Calcola l'errore medio sui fold della cross-validation\n",
    "cv_results = grid_result.cv_results_\n",
    "#mean_train_rmse = np.mean(np.sqrt(-cv_results['mean_train_score']))  # RMSE\n",
    "mean_val_rmse = np.mean(np.sqrt(-cv_results['mean_test_score']))  # RMSE\n",
    "\n",
    "# Stampa i risultati dell'RMSE\n",
    "#print(\"Mean Train RMSE:\", mean_train_rmse)\n",
    "print(\"Mean Validation RMSE:\", mean_val_rmse)\n",
    "\n",
    "# Calcola il MAE medio sui fold della cross-validation\n",
    "#mean_train_mae = np.mean(-cv_results['mean_train_score'])  # MAE\n",
    "mean_val_mae = np.mean(-cv_results['mean_test_score'])  # MAE\n",
    "\n",
    "# Stampa i risultati del MAE\n",
    "#print(\"Mean Train MAE:\", mean_train_mae)\n",
    "print(\"Mean Validation MAE:\", mean_val_mae)\n",
    "\n",
    "# Addestra il miglior modello sul dataset completo\n",
    "best_model.fit(df.drop(columns=['avg_tens']), df['avg_tens'])\n",
    "\n",
    "# Applica il modello sul dataset df_test\n",
    "test_predictions = best_model.predict(df_test.drop(columns=['avg_tens']))\n",
    "test_error_rmse = np.sqrt(mean_squared_error(df_test['avg_tens'], test_predictions))  # RMSE\n",
    "test_error_mae = mean_absolute_error(df_test['avg_tens'], test_predictions)  # MAE\n",
    "\n",
    "# Stampa l'RMSE e il MAE sul dataset df_test\n",
    "print(\"Test RMSE on df_test:\", test_error_rmse)\n",
    "print(\"Test MAE on df_test:\", test_error_mae)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58a83047-6608-498d-bb83-325a1b1316e4",
   "metadata": {},
   "source": [
    "df_test = df_test.astype('float64')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1ef2278-c374-408b-8de9-1c396cc7cc70",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Definire il numero di suddivisioni desiderate per la cross-validation\n",
    "n_splits = 5\n",
    "\n",
    "# Inizializzare il modello di regressione lineare\n",
    "model = LinearRegression()\n",
    "\n",
    "# Inizializzare lo splitter per la cross-validation di tipo sliding window\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Inizializzare le liste per memorizzare gli errori su train e validation\n",
    "train_errors = []\n",
    "val_errors = []\n",
    "\n",
    "# Eseguire la cross-validation\n",
    "for train_index, val_index in tscv.split(df):\n",
    "    X_train, X_val = df.iloc[train_index], df.iloc[val_index]\n",
    "    y_train, y_val = df['avg_tens'].iloc[train_index], df['avg_tens'].iloc[val_index]\n",
    "    \n",
    "    # Addestrare il modello sul training set\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calcolare le previsioni su training e validation set\n",
    "    train_predictions = model.predict(X_train)\n",
    "    val_predictions = model.predict(X_val)\n",
    "    \n",
    "    # Calcolare l'errore sul training set e sul validation set\n",
    "    train_error = mean_squared_error(y_train, train_predictions)\n",
    "    val_error = mean_squared_error(y_val, val_predictions)\n",
    "    \n",
    "    # Aggiungere gli errori alle liste\n",
    "    train_errors.append(train_error)\n",
    "    val_errors.append(val_error)\n",
    "\n",
    "# Calcolare la media degli errori su train e validation\n",
    "mean_train_error = np.mean(train_errors)\n",
    "mean_val_error = np.mean(val_errors)\n",
    "\n",
    "# Stampare i risultati\n",
    "print(\"Mean Train Error:\", mean_train_error)\n",
    "print(\"Mean Validation Error:\", mean_val_error)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe57e4-0b3d-442a-a94c-4729dd1dd8c9",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
